%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath} % Math env.

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Kernel Learning]{Kernel Learning And Its Application In Nonlinear Support Vector Machines} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Anastasia S., Sven N., Joshua S.} % Your name
\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Otto-Friedrich-University Bamberg \\ % Your institution for the title page
\medskip
\textit{anastasia.sinitsyna@stud.uni-bamberg.de \\}
\textit{sven-jason-waldemar.nekula@stud.uni-bamberg.de \\} % Your email address
\textit{joshua-guenter.simon@stud.uni-bamberg.de} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introduction} 
%------------------------------------------------


%------------------------------------------------
\subsection{Linearly separable data classes}

\begin{frame}{}
    \frametitle{Linearly separable data classes}
    First, let's consider a given data set $\mathcal{X}$ of labeled points (inputs) with individual labels $y_i \in \left\{ -1,1 \right\}$, e.g. $(x_1,y_1), ..., (x_m, y_m) \in \mathcal{X} \times \left\{ -1,1 \right\}$. \\~\\
    
    Our goal is to implement a classification method, which is able to classify new and unlabeld data points with the right or "best" label. \\~\\
\end{frame}


\begin{frame}{}
	\frametitle{Linearly separable data classes}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{img/LinearSVM_Data.png}
		\caption{ An example for linearly separable data.}
		\label{fig:lineardata}
	\end{figure} 
\end{frame}


\begin{frame}{}
    \frametitle{Linearly separable data classes}
    In machine learning, a well established classification method are the so called \textbf{Support Vector Machines} (SVM). Developed by Vladimir Vapnik and his coworkers in the 1990s, SVMs are still a relevent topic and an even more powerfull tool for \textbf{classification} and \textbf{regression}.
\end{frame}


%------------------------------------------------
\subsection{Similarity, dot product and vector norm}

\begin{frame}{}
    \frametitle{Similarity}
    To perform a classification, a similarity measure is needed. Finding a suitable measure is a core problem of machine learning. For now let's consider 
    \begin{equation}
        \begin{aligned}
            k: \mathcal{X} \times \mathcal{X} & \rightarrow \mathbb{R} \\
            (x, x') & \mapsto k(x, x')
        \end{aligned}
    \end{equation}
    where $k$ is a function that, given two patterns $x$ and $x'$, returns a real number characterizing their similarity. This function $k$ is called a \textbf{kernel}. Unless stated otherwise, $k(x, x') = k(x', x)$.
\end{frame}


\begin{frame}{}
    \frametitle{Dot product and vector norm}
    A simple type of similarity measure is a \textbf{dot product}. Given two vectors $x, x' \in \mathbb{R}^n$ the canonical dot product is defined as
    \begin{equation}
        \langle x,x' \rangle = \sum_{i = 1}^{n} x_i x'_i.
    \end{equation}
    Futhermore this allows a calculation of the \textbf{norm} (length) of a single vector $x$ as 
    \begin{equation}
        \left\lVert x \right\rVert = \sqrt{\langle x,x \rangle}.
    \end{equation}
\end{frame}


%------------------------------------------------
\subsection{Hyperplane classifiers - Solving an optimization problem}

\begin{frame}{}
    \frametitle{Hyperplane classifiers}
    The underlying learning algorithm of SVMs yields to find a hyperplane in some dot product space $\mathcal{H}$, which separates the data. A hyperplane of the form
    \begin{equation}
        \langle w,x \rangle + b = 0
    \end{equation}
    where $w \in \mathcal{H}, b \in \mathbb{R}$ shall be considered [p.11 Kernel Learning Book]. Futhermore decision functions 
    \begin{equation}
        f(x) = sgn \left( \langle w,x \rangle + b \right)
    \end{equation}
    can be asigned.
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - A constrained optimization problem}
    The \textbf{optimal hyperplane} can be calculated by finding the normal vector that leads to the largest margin. Thus we need to solve the optimization problem
    \begin{equation} \label{eq:1}
        \begin{aligned}
            \min_{w \in \mathcal{H}, b \in \mathbb{R}} \quad & \tau (w) = \frac{1}{2} \lVert w \rVert^2 \\
            \textrm{subject to} \quad & y_{i} \left( \langle w,x \rangle + b \right) \geq 1 \text{ } \forall i = {1, \dots, m}. 
        \end{aligned}
    \end{equation}
    The constraints in \eqref{eq:1} ensure that $f(x_i)$ will be $+1$ for $y_i = +1$ and $-1$ for  $y_i = -1$. The $\geq 1$ on the right hand side of the constraints effectively fixes the scaling of $w$. This leads to the maximum margin hyperplane. A detailed explanation can be found in [Chap 7, Kernel Learning Book].
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - Lagrangian}
    The constrained optimization problem in \eqref{eq:1} can be re-written using the method of Lagrange multipliers. This leads to the Lagrangian
    \begin{equation} \label{eq:2}
        L(w,b,\alpha) = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^{m} \alpha_i \left( y_{i} \left( \langle w,x \rangle + b \right) - 1 \right)
    \end{equation}
    subject to $\alpha_i \geq 0 \text{ } \forall i = {1, \dots, m}$. Here, $\alpha_i$ are the Lagrange multipliers. The Lagrangian $L$ has to be minimized with respect to the primal variables $w$ and $b$ and maximized with respect to the dual variables $\alpha_i$ (in other words, a saddle point has to be found).
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - KKT conditions}
    The Karush-Kuhn-Tucker (KKT) complementarity conditions of optimization theory state, that at the saddle point, the derivatives of $L$ with respect to the
    primal variables must vanish, since
    \begin{equation}
        \frac{\partial}{\partial b} L(w,b,\alpha) = 0 \text{ and } \frac{\partial}{\partial w} L(w,b,\alpha) = 0
    \end{equation}
    leads to
    \begin{equation} \label{eq:3}
        \sum_{i=1}^{m} \alpha_i y_i = 0 \text{ and } w = \sum_{i=1}^{m} \alpha_i y_i x_i.
    \end{equation}
    The solution vector $w$ thus has an expansion in terms of a subset of the training patterns, namely those patterns with non-zero $\alpha_i$, called Support Vectors (SVs).
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - Dual optimization problem}
    We can again re-write our optimization problem by substituting \eqref{eq:3} into the Lagrangian \eqref{eq:2} to eliminate the primal variables. This yields the dual optimization problem, which is usually solved in practice
    \begin{equation} \label{eq:4}
        \begin{aligned}
            \max_{\alpha \in \mathbb{R}^m} \quad & W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y_i y_j \langle x_i,x_j \rangle \\
            \textrm{subject to} \quad & \alpha_i \geq 0 \text{ } \forall i = {1, \dots, m} \text{ and } \sum_{i=1}^{m} \alpha_i y_i = 0. 
        \end{aligned}
    \end{equation}
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - Dual optimization problem}
    Finally, the decision function can be re-written using \eqref{eq:3} as
    \begin{equation}
        f(x) = sgn \left( \sum_{i=1}^{m} \alpha_i y_i \langle x,x_i \rangle + b \right),
    \end{equation}
    where $b$ can be computed by exploiting $\alpha_i \left[ y_i \left( \langle x_i,w \rangle + b \right) - 1 \right] = 0$, which follows from the KKT conditions.
\end{frame}



%------------------------------------------------
\section{Linear SVMs}
%------------------------------------------------

\subsection{Maximum margin separator}

\begin{frame}{}
    \frametitle{Maximum margin separator}
    We now have all the theoretical background to go back to our inital classification problem. We can implement a SVM as a maximum margin separator for the given data set $\mathcal{X}$. \\~\\  
\end{frame}

\begin{frame}{}
	\frametitle{Maximum margin separator}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{img/LinearSVM_complete.png}
		\caption{Implementation of a SVM with linearly separable data.}
		\label{fig:linearsvm}
	\end{figure}
	 
\end{frame}

%------------------------------------------------

\subsection{Limitations}

\begin{frame}{}
    \frametitle{Limitations}
    Let's consider the following data set. \\~\\ 
    - Plot of nonlinear separable data - 
\end{frame}



%------------------------------------------------
\section{Nonlinear SVMs}
%------------------------------------------------

\subsection{The kernel trick}
\subsection{Solving nonlinear separable classification problemes}

%------------------------------------------------



%------------------------------------------------
\section{More kernel applications}
%------------------------------------------------


%------------------------------------------------





%------------------------------------------------
% Template things....
%------------------------------------------------


\begin{frame}
\frametitle{Paragraphs of Text}
Sed iaculis dapibus gravida. Morbi sed tortor erat, nec interdum arcu. Sed id lorem lectus. Quisque viverra augue id sem ornare non aliquam nibh tristique. Aenean in ligula nisl. Nulla sed tellus ipsum. Donec vestibulum ligula non lorem vulputate fermentum accumsan neque mollis.\\~\\

Sed diam enim, sagittis nec condimentum sit amet, ullamcorper sit amet libero. Aliquam vel dui orci, a porta odio. Nullam id suscipit ipsum. Aenean lobortis commodo sem, ut commodo leo gravida vitae. Pellentesque vehicula ante iaculis arcu pretium rutrum eget sit amet purus. Integer ornare nulla quis neque ultrices lobortis. Vestibulum ultrices tincidunt libero, quis commodo erat ullamcorper id.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Bullet Points}
\begin{itemize}
\item Lorem ipsum dolor sit amet, consectetur adipiscing elit
\item Aliquam blandit faucibus nisi, sit amet dapibus enim tempus eu
\item Nulla commodo, erat quis gravida posuere, elit lacus lobortis est, quis porttitor odio mauris at libero
\item Nam cursus est eget velit posuere pellentesque
\item Vestibulum faucibus velit a augue condimentum quis convallis nulla gravida
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Blocks of Highlighted Text}
\begin{block}{Block 1}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.
\end{block}

\begin{block}{Block 2}
Pellentesque sed tellus purus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vestibulum quis magna at risus dictum tempor eu vitae velit.
\end{block}

\begin{block}{Block 3}
Suspendisse tincidunt sagittis gravida. Curabitur condimentum, enim sed venenatis rutrum, ipsum neque consectetur orci, sed blandit justo nisi ac lacus.
\end{block}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Multiple Columns}
\begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment

\column{.45\textwidth} % Left column and width
\textbf{Heading}
\begin{enumerate}
\item Statement
\item Explanation
\item Example
\end{enumerate}

\column{.5\textwidth} % Right column and width
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.

\end{columns}
\end{frame}

%------------------------------------------------


\begin{frame}
\frametitle{Table}
\begin{table}
\begin{tabular}{l l l}
\toprule
\textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
\midrule
Treatment 1 & 0.0003262 & 0.562 \\
Treatment 2 & 0.0015681 & 0.910 \\
Treatment 3 & 0.0009271 & 0.296 \\
\bottomrule
\end{tabular}
\caption{Table caption}
\end{table}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Theorem}
\begin{theorem}[Mass--energy equivalence]
$E = mc^2$
\end{theorem}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
\frametitle{Verbatim}
\begin{example}[Theorem Slide Code]
\begin{verbatim}
\begin{frame}
\frametitle{Theorem}
\begin{theorem}[Mass--energy equivalence]
$E = mc^2$
\end{theorem}
\end{frame}\end{verbatim}
\end{example}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Figure}
Uncomment the code on this slide to include your own image from the same directory as the template .TeX file.
%\begin{figure}
%\includegraphics[width=0.8\linewidth]{test}
%\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
\frametitle{Citation}
An example of the \verb|\cite| command to cite within the presentation:\\~

This statement requires citation \cite{p1}.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
\bibitem[Smith, 2012]{p1} John Smith (2012)
\newblock Title of the publication
\newblock \emph{Journal Name} 12(3), 45 -- 678.
\end{thebibliography}
}
\end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kernel Learning And Its Application In Nonlinear Support Vector Machines\n",
    "\n",
    "Sven Nekula (Otto-Friedrich-University Bamberg, sven.nekula@stud.uni-bamberg.de) <br />\n",
    "Joshua Simon (Otto-Friedrich-University Bamberg, joshua-gunter.simon@stud.uni-bamberg.de)\n",
    "\n",
    "## Overview\n",
    "1. Introduction\n",
    "    * Linear separable data classes\n",
    "    * Similarity, dot product and vector norm\n",
    "    * Hyperplane classifiers: Solving an optimization problem\n",
    "2. Linear SVMs\n",
    "    * Maximum margin separator\n",
    "    * Limitations\n",
    "3. Nonlinear SVMs\n",
    "    * The kernel trick\n",
    "    * Solving a nonlinear sepqarable classification problem\n",
    "4. More kernel applications\n",
    "5. Conclusion\n",
    "\n",
    "## Python Moduls"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## Linear separable data classes\n",
    "First, let's consider a given data set $\\mathcal{X}$ of labeled points (inputs) with individual labels $y_i \\in \\left\\{ -1,1 \\right\\}$, e.g. $(x_1,y_1), ..., (x_m, y_m) \\in \\mathcal{X} \\times \\left\\{ -1,1 \\right\\}$. Our goal is to implement a classification method, which is able to classify new and unlabeld data points with the right or \"best\" label. In machine learning, a well established classification method are the so called __Support Vector Machines__ (SVM). Developed by Vladimir Vapnik and his coworkers in the 1990s, SVMs are still a relevent topic and an even more powerfull tool for __classification__ and __regression__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Learning And Its Application In Nonlinear Support Vector Machines\n",
    "\n",
    "Sven Nekula (Otto-Friedrich-University Bamberg, sven.nekula@stud.uni-bamberg.de) <br />\n",
    "Joshua Simon (Otto-Friedrich-University Bamberg, joshua-gunter.simon@stud.uni-bamberg.de) <br />\n",
    "Anastasia Sinitsyna (Otto-Friedrich-University Bamberg, anastasia.sinitsyna@stud.uni-bamberg.de)\n",
    "\n",
    "## Overview\n",
    "1. Introduction\n",
    "    * Linear separable data classes\n",
    "    * Similarity, dot product and vector norm\n",
    "    * Hyperplane classifiers: Solving an optimization problem\n",
    "2. Linear SVMs\n",
    "    * Maximum margin separator\n",
    "    * Limitations\n",
    "3. Nonlinear SVMs\n",
    "    * The kernel trick\n",
    "    * Solving a nonlinear sepqarable classification problem\n",
    "4. More kernel applications\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Moduls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## Linear separable data classes\n",
    "First, let's consider a given data set $\\mathcal{X}$ of labeled points (inputs) with individual labels $y_i \\in \\left\\{ -1,1 \\right\\}$, e.g. $(x_1,y_1), ..., (x_m, y_m) \\in \\mathcal{X} \\times \\left\\{ -1,1 \\right\\}$. Our goal is to implement a classification method, which is able to classify new and unlabeld data points with the right or \"best\" label. In machine learning, a well established classification method are the so called __Support Vector Machines__ (SVM). Developed by Vladimir Vapnik and his coworkers in the 1990s, SVMs are still a relevent topic and an even more powerfull tool for __classification__ and __regression__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE TO GENERATE LINEAR SEPARABLE DATA + PLOT\n",
    "\n",
    "def generate_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "To perform a classification, a similarity measure is needed. Finding a suitable measure is a core problem of machine learning. For now let's consider\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        k: \\mathcal{X} \\times \\mathcal{X} & \\rightarrow \\mathbb{R} \\\\\n",
    "        (x, x') & \\mapsto k(x, x')\n",
    "    \\end{aligned}\n",
    "\\label{eq:kernel} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $k$ is a function that, given to patterns $x$ and $x'$, returns a real number characterizing their similarity. \n",
    "This function $k$ is called a **kernel**. Unless stated otherwise, $k(x, x') = k(x', x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product and vector norm\n",
    "A simple type of similarity measure is a **dot product**. Given two vectors $x, x' \\in \\mathbb{R}^n$ the canonical dot product is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle x,x' \\rangle = (x')^T x = \\sum_{i = 1}^{n} [x]_i [x']_i,\n",
    "\\label{eq:dotproduct} \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\left[x\\right]_i$ denotes the $i$th entry of $x$. Futhermore this allows a calculation of the **norm** as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left\\lVert x \\right\\rVert = \\sqrt{\\langle x,x \\rangle}.\n",
    "\\label{eq:nrom} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Given a vector space $\\mathcal{V}$ (mostly over $\\mathbb{R}$ or $\\mathbb{C}$) and a dot product, one can define a so called **dot product space** or **Pre-Hilbert space** $\\mathcal{H}$, where every pair of elements $x, x' \\in \\mathcal{V}$ is assigned to a scalar value, the dot product of $x$ and $x'$\n",
    "\n",
    "More properties of vector spaces, dot products and norms can be found in **ADD SOURCE LIESEN, 2015**-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperplane classifiers\n",
    "\n",
    "The underlying learning algorithm of SVMs yields to find a hyperplane in some dot product space $\\mathcal{H}$, which separates the data. A hyperplane of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle w,x \\rangle + b = 0\n",
    "\\label{eq:hyperplane} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where $w \\in \\mathcal{H}, b \\in \\mathbb{R}$ shall be considered **ADD SOURCE SCHÖLLKOPF**. $w$ is a weight vector, while $b$ is a bias.\n",
    "\n",
    "Futhermore decision functions\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = sgn \\left( \\langle w,x \\rangle + b \\right)\n",
    "\\label{eq:decfun1} \\tag{5}\n",
    "\\end{equation}\n",
    "    \n",
    "can be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE WITH PLOT CONTAINING POSSIBLE HYPERPLANES\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A contrained optimization problem\n",
    "The **optimal hyperplane** can be calculated by finding the normal vector $w$ that leads to the largest margin. Thus we need to solve the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w \\in \\mathcal{H}, b \\in \\mathbb{R}} \\quad & \\tau (w) = \\frac{1}{2} \\lVert w \\rVert^2 \\\\\n",
    "        \\textrm{subject to} \\quad & y_{i} \\left( \\langle w,x \\rangle + b \\right) \\geq 1 \\text{ } \\forall i = {1, \\dots, m}. \n",
    "    \\end{aligned}\n",
    "\\label{eq:objfun} \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "The constraints in $\\eqref{eq:objfun}$ ensure that $f(x_i)$ will be $+1$ for $y_i = +1$ and $-1$ for  $y_i = -1$. The $\\geq 1$ on the right hand side of the constraints effectively fixes the scaling of $w$. This leads to the maximum margin hyperplane. A detailed explanation can be found in **ADD SOURCE SCHÖLLKOPF**.\n",
    "\n",
    "### Lagrangian\n",
    "The constrained optimization problem in $\\eqref{eq:objfun}$ can be re-written using the method of Lagrange multipliers. This leads to the Lagrangian\n",
    "\n",
    "\\begin{equation}\n",
    "    L(w,b,\\alpha) = \\frac{1}{2} \\lVert w \\rVert^2 - \\sum_{i=1}^{m} \\alpha_i \\left( y_{i} \n",
    "    \\left( \\langle w,x \\rangle + b \\right) - 1 \\right)\\\\\n",
    "    \\textrm{subject to} \\quad \\alpha_i \\geq 0 \\text{ } \\forall i = {1, \\dots, m}.\n",
    "\\label{eq:Lagrangian} \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\alpha_i$ are the Lagrange multipliers. The Lagrangian $L$ has to be minimized with respect to the primal variables $w$ and $b$ and maximized with respect to the dual variables $\\alpha_i$ (in other words, a saddle point has to be found).\n",
    "\n",
    "### KKT conditions\n",
    "The Karush-Kuhn-Tucker (KKT) complementarity conditions of optimization theory state, that at the saddle point, the derivatives of $L$ with respect to the primal variables must vanish, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial b} L(w,b,\\alpha) = 0 \\text{ and } \\frac{\\partial}{\\partial w} L(w,b,\\alpha) = 0\n",
    "\\label{eq:KKT} \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "leads to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{m} \\alpha_i y_i = 0 \\text{ and } w = \\sum_{i=1}^{m} \\alpha_i y_i x_i.\n",
    "\\label{eq:KKT2} \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "The solution vector $w$ thus has an expansion in terms of a subset of the training patterns, namely those patterns with non-zero $\\alpha_i$, called Support Vectors (SVs).\n",
    "\n",
    "### Dual optimization problem\n",
    "We can again re-write our optimization problem by substituting $\\eqref{eq:KKT2}$ into the Lagrangian $\\eqref{eq:Lagrangian}$ to eliminate the primal variables. This yields the dual optimization problem, which is usually solved in practice\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "            \\max_{\\alpha \\in \\mathbb{R}^m} \\quad & W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} \\alpha_i \n",
    "            \\alpha_j y_i y_j \\langle x_i,x_j \\rangle \\\\\n",
    "            \\textrm{subject to} \\quad & \\alpha_i \\geq 0 \\text{ } \\forall i = {1, \\dots, m} \\text{ and } \\sum_{i=1}^{m} \n",
    "            \\alpha_i y_i = 0. \n",
    "    \\end{aligned}\n",
    "\\label{eq:DuOpPr} \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "The dual optimization problem $\\eqref{eq:DuOpPr}$ is a **convex quadratic programming problem** and therefore can be solved by using standard optimization techniques. \n",
    "\n",
    "Finally, the decision function can be re-written using $\\eqref{eq:KKT2}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = sgn \\left( \\sum_{i=1}^{m} \\alpha_i y_i \\langle x,x_i \\rangle + b \\right),\n",
    "\\label{eq:decfun2} \\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "where $b$ can be computed by exploiting $\\alpha_i \\left[ y_i \\left( \\langle x_i,w \\rangle + b \\right) - 1 \\right] = 0$, which follows from the KKT conditions.\n",
    "\n",
    "\n",
    "Details on mathematical optimization and convex constrained problems can be found in **ADD SOURCE JARRE**. Explanations on dealing with nonlinear problems are given in **ADD SOURCE REINHARDT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum margin seperator\n",
    "We now have all the theoretical background to go back to our inital classification problem. We can implement a SVM as a maximum margin separator for the given data set $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE WITH LINEAR SVM + PLOT\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "Let's consider following data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE + PLOT WITH NOISY DATA\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Hyperplanes\n",
    "We introduce a slack variable\n",
    "\n",
    "\\begin{equation}\n",
    "    \\xi_{ i } \\geq 0 \\text{ } \\forall i = {1, \\dots, m}\n",
    "\\label{eq:Xi} \\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "in the simplest case, this leads to \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w \\in \\mathcal{H}, \\xi \\in \\mathbb{R}^{n}} \\quad & \\tau (w, \\xi) = \\frac{1}{2} \\lVert w \\rVert^2 + \n",
    "        \\frac{C}{m} \n",
    "        \\sum_{i=1}^{m} \\xi_{i} \\\\\n",
    "        \\textrm{subject to} \\quad & y_{i} \\left( \\langle w,x \\rangle + b \\right) \\geq 1 - \\xi_{i} \\text{ } \n",
    "        \\forall i = {1, \\dots, m}. \n",
    "    \\end{aligned}\n",
    "\\label{eq:objfun2} \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "By making $\\xi_{i}$ large enough, the constraint can always be met, which is why we penalize them in the objective function with $\\frac{C}{m}$, where $C \\in \\mathbb{R}$ is a regularization parameter.\n",
    "\n",
    "Our dual optimization problem also gets re-written as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\max_{\\alpha \\in \\mathbb{R}^m} \\quad & W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} \\alpha_i \n",
    "        \\alpha_j y_i y_j \\langle x_i,x_j \\rangle \\\\\n",
    "        \\textrm{subject to} \\quad & 0 \\leq \\alpha_i \\leq \\frac{C}{m} \\text{ } \\forall i = {1, \\dots, m} \\text{ and } \n",
    "        \\sum_{i=1}^{m} \\alpha_i y_i = 0. \n",
    "    \\end{aligned}\n",
    "\\label{eq:DuOpPr2} \\tag{14}\n",
    "\\end{equation}\n",
    "\n",
    "This classifier is referred to as C-SV classifier and can be used to prevent overfitting by allowing the classifier to make false classifications. \\\\\n",
    "More classifiers using soft margins can be found in **ADD SOURCE SCHÖLLKOPF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE WITH SOFT MARGIN SVMs USING DIFFERENT Cs\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider following data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE WITH NON LINEARLY SEPARABLE DATA (MOON/CIRCLE)\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you try to separate them linearly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE WITH LINEAR KERNEL SVM\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nonlinear SVMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The kernel trick\n",
    "To extend the introduced SVM algorithm, we can substitute ([11](#mjx-eqn-decfun2)) by applying a kernel of the form\n",
    "\n",
    "**FIX REFERENCE?**\n",
    "\n",
    "\\begin{equation}\n",
    "    k(x,x') = \\langle \\Phi (x), \\Phi (x') \\rangle\n",
    "\\tag{15}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\Phi: \\mathcal{X} & \\rightarrow \\mathcal{H} \\\\\n",
    "        (x) & \\mapsto \\Phi (x)\n",
    "    \\end{aligned}\n",
    "\\tag{16}\n",
    "\\end{equation}\n",
    "\n",
    "is a function that maps an input from $ \\mathcal{X} $ into a dot product space $ \\mathcal{H} $. This is referred to as the **kernel trick**.\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = sgn \\left( \\sum_{i=1}^{m} \\alpha_i y_i \\langle \\Phi (x), \\Phi (x_i) \\rangle + b \\right) \\\\ \n",
    "    = sgn \\left( \\sum_{i=1}^{m} \\alpha_i y_i k(x,x_i) + b \\right)\n",
    "\\tag{17}\n",
    "\\end{equation}\n",
    "\n",
    "and the optimization problem\n",
    "\n",
    "\\begin{equation} \\label{eq:6}\n",
    "    \\begin{aligned}\n",
    "            \\max_{\\alpha \\in \\mathbb{R}^m} \\quad & W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} \n",
    "            \\alpha_i \\alpha_j y_i y_j k(x, x_i) \\\\\n",
    "            \\textrm{subject to} \\quad & \\alpha_i \\geq 0 \\text{ } \\forall i = {1, \\dots, m} \\text{ and } \\sum_{i=1}^{m} \n",
    "            \\alpha_i y_i = 0. \n",
    "    \\end{aligned}\n",
    "\\tag{18}\n",
    "\\end{equation}\n",
    "\n",
    "The $m \\times m$ Matrix $K$ with elements $K_{ij} = k(x_i, x_j)$ is called the **Gram matrix** r kernel matrix) of $k$.\n",
    "\n",
    "A kernel $k$ is called **sitive definite kernel** when the Gram matrix $K$ is positive definite.\n",
    "\n",
    "As stated in **ADD SOURCE SCHÖLLKOPF**: \\textit{Given an algorithm which is formulated in terms of a positive definite kernel $k$, one can construct an alternative algorithm by replacing $k$ by another positive definite kernel $\\tilde{k}$.}\n",
    "\n",
    "The kernel trick can be applied since all feature vectors only occurred in dot products. A more precise explanation can be found in **ADD SOURCE SCHÖLLKOPF**.\n",
    "\n",
    "## A suitable kernel\n",
    "Going back to our problem of non linearly separable data, we can use a kernel function of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    k(x, x') = \\exp \\left( - \\frac{\\left\\lVert x - x' \\right\\rVert^2}{2 \\sigma^2} \\right),\n",
    "\\tag{19}\n",
    "\\end{equation}\n",
    "\n",
    "a so called \\textbf{Gaussian radial basis function} (GRBF or RBF kernels) with $ \\sigma > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE FOR 3D MAPPED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE FOR NONLINEAR SVMs + PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of kernels\n",
    "An overview of common kernels:\n",
    "\n",
    "* **Linear**: $k(x,x') = \\langle x, x' \\rangle$\n",
    "* **Polynomial**:  $k(x,x') = \\langle x, x' \\rangle^{d}, d \\in \\mathbb{N}$\n",
    "* **Inhomogeneous Polynomial**: $k(x,x') = \\left( \\langle x, x' \\rangle + c \\right)^{d}, d \\in \\mathbb{N}, \n",
    "    c \\geq 0$\n",
    "* **Gaussian**: $k(x, x') = \\exp \\left( - \\frac{\\left\\lVert x - x' \\right\\rVert^2}{2 \\sigma^2} \\right), \n",
    "    \\sigma > 0$\n",
    "* **Sigmoid**: $k(x, x') = \\tanh \\left( \\kappa \\langle x, x' \\rangle + \\vartheta \\right), \\kappa > 0, \\vartheta < 0$\n",
    "\n",
    "These kernels are implemented in the Python modul scikit-learn \\texttt{sklearn.svm} based on the libsvm implementation in C\\texttt{++} by Chih-Chung Chang and Chih-Jen Lin \\cite{libsvm}.\n",
    "\n",
    "## More kernel applications\n",
    "Some interesting kernel applications:\n",
    "* Image recognition/classification (with SVMs) for example in \n",
    "    * Handwriting recognition\n",
    "    * Tumor detection\n",
    "* Computer vision and computer graphics, 3D reconstruction\n",
    "* Kernel principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
